{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import sklearn\n",
    "import os\n",
    "import copy\n",
    "\n",
    "from IPython.display import Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA = torch.cuda.is_available()\n",
    "RANDOM_SEED = 1\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_EPOCHS = 15\n",
    "\n",
    "print(f\"CUDA: {CUDA}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Extracted features, scores, metadata, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"training_set/scores_v2.csv\").set_index(\"video_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from features.video import load_C3D_features\n",
    "from features.image import load_ResNet152_features, load_LBP_features, load_HOG_features\n",
    "from features.audio import load_VGGish_features\n",
    "from features.emotion import load_Emotion_features, extract_emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"emotion\"] = load_Emotion_features(data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"lbp\"] = load_LBP_features(data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"hog\"] = load_HOG_features(data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"c3d\"] = load_C3D_features(data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"resnet152\"] = load_ResNet152_features(data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"vggish\"] = load_VGGish_features(data.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Prep\n",
    "\n",
    "Khosla points out that the memorability score used in Isola's paper did not take into account the memory retention duration.\n",
    "Cohendet utilized the same idea as Khosla, which involved a decay rate\n",
    "\n",
    "$$\\alpha \\leftarrow \n",
    "\\frac{\\sum^N_{i=1}\\frac{1}{n^{(i)}} \\sum^{n^{(i)}}_{j=1} \\log(\\frac{t^{(i)}_j}{T})[x^{(i)}_j - m^{(i)}_T] }\n",
    "{\\sum^N_{i=1}\\frac{1}{n^{(i)}} \\sum^{n^{(i)}}_{j=1}[ \\log(\\frac{t^{(i)}_j}{T})]^2}\n",
    "$$\n",
    "\n",
    "to calculate memorability \n",
    "$$\n",
    "m_T^{(i)} \\leftarrow\n",
    "\\frac{1}{n^{(i)}} \\sum^{n^{(i)}}_{j=1}[x^{(i)}_j - \\alpha \\log(\\frac{t_j^{(i)}}{T})]\n",
    "$$\n",
    "\n",
    "where we have $n^{(i)}$ observations for image $i$ given by $x^{(i)} \\in {0,1}$ and $t^{(i)}_j$ where $x_j=1$  implies that the image repeat was correctly detected when it shown after time $t_j$\n",
    "\n",
    "IDEA: potentially explore calculating $\\alpha$ per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from target_augmentation import add_position_delta, calculate_alpha_and_memorability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = pd.read_csv(\"training_set/short_term_annotations_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_position_delta(annotations)\n",
    "\n",
    "print(\"Average t:\", np.mean(annotations[\"t\"]))\n",
    "\n",
    "# We use approximately the average_t to calculate T as the memorability in question\n",
    "big_t = int(np.around(np.mean(annotations[\"t\"])))\n",
    "label = f\"m_{big_t}\"\n",
    "print(f\"Calculating adjusted value for {label}\")\n",
    "alpha, data[label] = calculate_alpha_and_memorability(annotations, T = big_t)\n",
    "print(f\"Alpha: {alpha}\")\n",
    "plt.scatter(data[\"part_1_scores\"], data[f\"m_{big_t}\"])\n",
    "plt.xlabel(\"Original memorability score\")\n",
    "plt.ylabel(f\"Adjusted memorability score ({label})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep\n",
    "\n",
    "Building datasets\n",
    "\n",
    "potentially weighting samples based on annotations?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head().iloc[0][\"lbp\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import split_training, build_matrixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = split_training(data)\n",
    "print(\"training:\",len(train_data))\n",
    "print(\"validation:\", len(valid_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pick Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"m_75\"\n",
    "features = \"vggish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, targets_train, video_ids_train = build_matrixes(train_data, target_name = target, feature_name = features)\n",
    "features_valid, targets_valid, video_ids_valid = build_matrixes(valid_data, target_name = target, feature_name = features)\n",
    "\n",
    "print(\"features_train shape:\", features_train.shape)\n",
    "print(\"targets_train shape:\", targets_train.shape)\n",
    "print(\"features_valid shape:\", features_valid.shape)\n",
    "print(\"features_valid shape:\", targets_valid.shape)\n",
    "total_features = len(features_train) + len(features_valid)\n",
    "unique, count = np.unique(np.concatenate([video_ids_valid, video_ids_train]), return_counts=True)\n",
    "print(\"Total features:\", total_features)\n",
    "print(\"Total videos:\", len(unique))\n",
    "print(\"Avg features per video:\", total_features / len(unique))\n",
    "print(\"Min features per video:\", min(count))\n",
    "print(\"Max features per video:\", max(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from train import train_two_layer_nn, train_svr\n",
    "\n",
    "def train_model(model_type, features_train, targets_train, features_valid, targets_valid):\n",
    "    if \"two_layer_nn\" == model_type:\n",
    "        model, train_losses, valid_losses = train_two_layer_nn(\n",
    "            features_train, targets_train, features_valid, targets_valid,\n",
    "            hidden_dim = 100,\n",
    "            num_epochs=NUM_EPOCHS,\n",
    "            cuda=CUDA,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            learning_rate=LEARNING_RATE\n",
    "        )\n",
    "\n",
    "        plt.plot(train_losses, label=\"Training Loss\")\n",
    "        plt.plot(valid_losses, label=\"Validation Loss\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    elif \"svr\" == model_type:\n",
    "        model = train_svr(features_train, targets_train)\n",
    "    else:\n",
    "        raise ValueError(f\"'{model_type}' is not a valid model type\")\n",
    "    return model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"two_layer_nn\"\n",
    "\n",
    "model = train_model(model_type, features_train, targets_train, features_valid, targets_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "\n",
    "Spearman's rank correlation, ROC curves, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import get_predictions\n",
    "\n",
    "predictions, actuals = get_predictions(model_type, model, features_valid, targets_valid, video_ids_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearman_rank, _ = stats.spearmanr(actuals, predictions)\n",
    "print(spearman_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "spearman_rank, _ = stats.spearmanr(actuals, predictions)\n",
    "print(\"SPEARMAN RANK:\",spearman_rank)\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(8,8))\n",
    "min_mem = min(np.min(actuals), np.min(predictions))\n",
    "max_mem = max(np.max(actuals), np.max(predictions))\n",
    "plt.scatter(actuals, predictions, label = f\"Spearman rank correlation = {spearman_rank}\")\n",
    "plt.plot([min_mem, max_mem], [min_mem, max_mem], label=\"1 to 1\")\n",
    "plt.title(f\"actual {target} vs predicted {target}\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"predictions\")\n",
    "plt.xlabel(\"actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "video-mem",
   "language": "python",
   "name": "video-mem"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
